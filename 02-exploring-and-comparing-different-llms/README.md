# Вивчення та порівняння різних LLM

[![Вивчення та порівняння різних LLM](./images/02-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)

> _Натисніть на зображення вище, щоб переглянути відео цього уроку_

У попередньому уроці ми побачили, як Генеративний ШІ змінює технологічний ландшафт, як працюють Великі Мовні Моделі (LLM) і як бізнес - як наш стартап - може застосовувати їх у своїх випадках використання та розвиватися! У цьому розділі ми розглянемо та порівняємо різні типи великих мовних моделей (LLM), щоб зрозуміти їхні переваги та недоліки.

Наступним кроком у подорожі нашого стартапу є дослідження поточного ландшафту LLM та розуміння того, які з них підходять для нашого випадку використання.

## Вступ

Цей урок охопить:

- Різні типи LLM у поточному ландшафті.
- Тестування, ітерація та порівняння різних моделей для вашого випадку використання в Azure.
- Як розгортати LLM.

## Цілі навчання

Після завершення цього уроку ви зможете:

- Вибрати правильну модель для вашого випадку використання.
- Зрозуміти, як тестувати, ітерувати та покращувати продуктивність вашої моделі.
- Знати, як бізнес розгортає моделі.

## Розуміння різних типів LLM

LLM можуть мати кілька категорій залежно від їхньої архітектури, навчальних даних та випадку використання. Розуміння цих відмінностей допоможе нашому стартапу вибрати правильну модель для сценарію та зрозуміти, як тестувати, ітерувати та покращувати продуктивність.

Існує багато різних типів моделей LLM, ваш вибір моделі залежить від того, для чого ви плануєте їх використовувати, ваших даних, скільки ви готові платити тощо.

Залежно від того, чи плануєте ви використовувати моделі для тексту, аудіо, відео, генерації зображень тощо, ви можете вибрати різні типи моделей.

- **Аудіо та розпізнавання мовлення**. Для цієї мети моделі типу Whisper є чудовим вибором, оскільки вони є універсальними і спрямовані на розпізнавання мовлення. Вони навчені на різноманітному аудіо і можуть виконувати багатомовне розпізнавання мовлення. Дізнайтеся більше про [моделі типу Whisper тут](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Генерація зображень**. Для генерації зображень DALL-E та Midjourney є двома дуже відомими варіантами. DALL-E пропонується Azure OpenAI. [Дізнайтеся більше про DALL-E тут](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) та також у Розділі 9 цього навчального плану.

- **Генерація тексту**. Більшість моделей навчені на генерації тексту, і у вас є великий вибір від GPT-3.5 до GPT-4. Вони мають різну вартість, при цьому GPT-4 є найдорожчою. Варто заглянути в [пісочницю Azure OpenAI](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst), щоб оцінити, які моделі найкраще відповідають вашим потребам з точки зору можливостей та вартості.

- **Мультимодальність**. Якщо ви шукаєте можливість обробки кількох типів даних на вході та виході, можливо, варто розглянути моделі, такі як [gpt-4 turbo with vision або gpt-4](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) - останні релізи моделей OpenAI - які здатні поєднувати обробку природної мови з візуальним розумінням, забезпечуючи взаємодію через мультимодальні інтерфейси.

Вибір моделі означає, що ви отримуєте деякі базові можливості, яких, однак, може бути недостатньо. Часто у вас є специфічні для компанії дані, про які вам потрібно якось повідомити LLM. Існує кілька різних варіантів підходу до цього, про це детальніше в наступних розділах.

### Фундаментальні моделі проти LLM

Термін Фундаментальна Модель був [запропонований дослідниками Стенфорда](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) і визначений як модель ШІ, що відповідає деяким критеріям, таким як:

- **Вони навчаються за допомогою навчання без учителя або самоконтрольованого навчання**, що означає, що вони навчаються на немаркованих мультимодальних даних, і для їх навчання не потрібна анотація або маркування даних людиною.
- **Це дуже великі моделі**, засновані на дуже глибоких нейронних мережах, навчених на мільярдах параметрів.
- **Вони зазвичай призначені служити 'фундаментом' для інших моделей**, що означає, що вони можуть використовуватися як відправна точка для побудови інших моделей зверху, що можна зробити шляхом тонкого налаштування.

![Фундаментальні моделі проти LLM](./images/FoundationModel.png?WT.mc_id=academic-105485-koreyst)

Джерело зображення: [Essential Guide to Foundation Models and Large Language Models | by Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Щоб далі прояснити цю відмінність, давайте візьмемо ChatGPT як приклад. Для побудови першої версії ChatGPT модель під назвою GPT-3.5 служила фундаментальною моделлю. Це означає, що OpenAI використовувала деякі специфічні для чату дані, щоб створити налаштовану версію GPT-3.5, яка спеціалізувалася на ефективній роботі в сценаріях спілкування, таких як чат-боти.

![Фундаментальна модель](./images/Multimodal.png?WT.mc_id=academic-105485-koreyst)

Джерело зображення: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Відкриті проти пропрієтарних моделей

Інший спосіб категоризації LLM - це їх відкритість чи пропрієтарність.

Моделі з відкритим кодом - це моделі, які доступні для громадськості і можуть використовуватися для генерації нового коду або для виправлення помилок в існуючому коді.

![Генерація тексту та коду](./images/Text.png?WT.mc_id=academic-105485-koreyst)

### Кодувальник-Декодувальник проти лише Декодувальника

Щоб поговорити про різні типи архітектур LLM, давайте використаємо аналогію.

Уявіть, що ваш менеджер дав вам завдання написати тест для студентів. У вас є два колеги; один відповідає за створення контенту, а інший за його перевірку.

Створювач контенту схожий на модель лише з Декодувальником, він може подивитися на тему і побачити, що ви вже написали, а потім написати курс на основі цього. Вони дуже добре пишуть захоплюючий та інформативний контент, але не дуже добре розуміють тему та навчальні цілі. Деякими прикладами моделей Декодувальника є моделі сімейства GPT, такі як GPT-3.

Рецензент схожий на модель лише з Кодувальником, вони дивляться на написаний курс та відповіді, помічаючи зв'язки між ними та розуміючи контекст, але вони не вміють генерувати контент. Прикладом моделі лише з Кодувальником був би BERT.

Уявіть, що у нас також може бути хтось, хто міг би створювати та перевіряти тест, це модель Кодувальник-Декодувальник. Деякими прикладами були б BART та T5.

### Сервіс проти Моделі

Тепер давайте поговоримо про різницю між сервісом та моделлю. Сервіс - це продукт, який пропонується Постачальником Хмарних Послуг, і часто є комбінацією моделей, даних та інших компонентів. Модель є основним компонентом сервісу і часто є фундаментальною моделлю, такою як LLM.

Сервіси часто оптимізовані для виробничого використання і часто простіші у використанні, ніж моделі, через графічний інтерфейс користувача. Однак сервіси не завжди доступні безкоштовно і можуть вимагати підписки або оплати за використання в обмін на використання обладнання та ресурсів власника сервісу, оптимізацію витрат та легке масштабування. Прикладом сервісу є [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), який пропонує тарифний план оплати за використання, що означає, що користувачі оплачують пропорційно тому, скільки вони використовують сервіс. Також Azure OpenAI Service пропонує корпоративний рівень безпеки та відповідальну структуру ШІ поверх можливостей моделей.

Моделі - це просто Нейронна Мережа з параметрами, вагами та іншим. Це дозволяє компаніям запускати їх локально, однак, потрібно буде купити обладнання, побудувати структуру для масштабування та купити ліцензію або використовувати модель з відкритим кодом. Модель, як LLaMA, доступна для використання, але вимагає обчислювальної потужності для запуску моделі.

## Як тестувати та ітерувати з різними моделями для розуміння продуктивності в Azure

Після того, як наша команда дослідила поточний ландшафт LLM та визначила деяких хороших кандидатів для своїх сценаріїв, наступним кроком є тестування їх на своїх даних та на своєму робочому навантаженні. Це ітеративний процес, який здійснюється через експерименти та вимірювання.
Більшість моделей, які ми згадували в попередніх параграфах (моделі OpenAI, моделі з відкритим кодом, як Llama2, та трансформери Hugging Face) доступні в [Каталозі Моделей](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) в [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) - це Хмарна Платформа, розроблена для розробників для створення генеративних ШІ-додатків та управління всім життєвим циклом розробки - від експериментів до оцінки - шляхом об'єднання всіх сервісів Azure AI в єдиний хаб з зручним графічним інтерфейсом користувача. Каталог Моделей в Azure AI Studio дозволяє користувачу:

- Знайти Фундаментальну Модель, яка цікавить, в каталозі - як пропрієтарну, так і з відкритим кодом, фільтруючи за завданням, ліцензією або назвою. Для покращення пошуку моделі організовані в колекції, як колекція Azure OpenAI, колекція Hugging Face тощо.

![Каталог моделей](./images/AzureAIStudioModelCatalog.png?WT.mc_id=academic-105485-koreyst)

- Переглянути картку моделі, включаючи детальний опис призначеного використання та навчальних даних, зразки коду та результати оцінки на внутрішній бібліотеці оцінок.

![Картка моделі](./images/ModelCard.png?WT.mc_id=academic-105485-koreyst)

- Порівняти показники ефективності між моделями та наборами даних, доступними в галузі, щоб оцінити, яка з них відповідає бізнес-сценарію, через панель [Показники Моделей](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst).

![Показники моделей](./images/ModelBenchmarks.png?WT.mc_id=academic-105485-koreyst)

- Точно налаштувати модель на користувацьких навчальних даних для покращення продуктивності моделі в конкретному робочому навантаженні, використовуючи можливості експериментування та відстеження Azure AI Studio.

![Точне налаштування моделі](./images/FineTuning.png?WT.mc_id=academic-105485-koreyst)

- Розгорнути оригінальну попередньо навчену модель або точно налаштовану версію до віддаленого виведення в реальному часі - керованих обчислень - або безсерверної кінцевої точки api - [оплата за використання](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) - щоб дозволити додаткам використовувати її.

![Розгортання моделі](./images/ModelDeploy.png?WT.mc_id=academic-105485-koreyst)

> [!ПРИМІТКА]
> Не всі моделі в каталозі наразі доступні для точного налаштування та/або розгортання з оплатою за використання. Перевірте картку моделі для деталей про можливості та обмеження моделі.

## Покращення результатів LLM

Ми дослідили з нашою командою стартапу різні види LLM та Хмарну Платформу (Azure Machine Learning), що дозволяє нам порівнювати різні моделі, оцінювати їх на тестових даних, покращувати продуктивність та розгортати їх на кінцевих точках виведення.

Але коли їм слід розглядати точне налаштування моделі замість використання попередньо навченої? Чи є інші підходи для покращення продуктивності моделі на конкретних робочих навантаженнях?

Існує кілька підходів, які бізнес може використовувати для отримання потрібних результатів від LLM. Ви можете вибрати різні типи моделей з різними ступенями навчання при розгортанні LLM у виробництві, з різними рівнями складності, вартості та якості. Ось деякі різні підходи:

- **Інженерія промптів з контекстом**. Ідея полягає в тому, щоб надати достатньо контексту при промпті, щоб забезпечити отримання потрібних відповідей.

- **Генерація з Розширенням Пошуку, RAG**. Ваші дані можуть існувати в базі даних або веб-кінцевій точці, наприклад, щоб забезпечити включення цих даних або їх підмножини на момент промпту, ви можете отримати відповідні дані та зробити їх частиною промпту користувача.

- **Точно налаштована модель**. Тут ви додатково навчили модель на власних даних, що призводить до того, що модель стає більш точною та відповідальною вашим потребам, але може бути дорогою.

![Розгортання LLM](./images/Deploy.png?WT.mc_id=academic-105485-koreyst)

Джерело зображення: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Інженерія промптів з контекстом

Попередньо навчені LLM дуже добре працюють на узагальнених завданнях природної мови, навіть при виклику їх коротким промптом, як речення для завершення або запитання – так зване навчання "з нуля".

Однак чим більше користувач може сформулювати свій запит, з детальним запитом та прикладами – Контекстом – тим точнішою та ближчою до очікувань користувача буде відповідь. У цьому випадку ми говоримо про навчання "з одного прикладу", якщо промпт включає лише один приклад, та навчання "з декількох прикладів", якщо він включає кілька прикладів.
Інженерія промптів з контекстом є найбільш економічно ефективним підходом для початку.

### Генерація з Розширенням Пошуку (RAG)

LLM мають обмеження в тому, що вони можуть використовувати лише дані, які були використані під час їх навчання, для генерації відповіді. Це означає, що вони нічого не знають про факти, які сталися після їх процесу навчання, і вони не можуть отримати доступ до непублічної інформації (як дані компанії).
Це можна подолати через RAG, техніку, яка розширює промпт зовнішніми даними у формі фрагментів документів, враховуючи обмеження довжини промпту. Це підтримується інструментами векторних баз даних (як [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)), які отримують корисні фрагменти з різних попередньо визначених джерел даних і додають їх до Контексту промпту.

Ця техніка дуже корисна, коли у бізнесу недостатньо даних, часу або ресурсів для точного налаштування LLM, але все ще бажає покращити продуктивність на конкретному робочому навантаженні та зменшити ризики фабрикацій, тобто містифікації реальності або шкідливого контенту.

### Точно налаштована модель

Точне налаштування - це процес, який використовує трансферне навчання для 'адаптації' моделі до подальшого завдання або для вирішення конкретної проблеми. На відміну від навчання з декількох прикладів та RAG, це призводить до створення нової моделі з оновленими вагами та упередженнями. Це вимагає набору навчальних прикладів, що складаються з одного входу (промпту) та пов'язаного з ним виходу (завершення).
Це був би кращий підхід, якщо:

- **Використання точно налаштованих моделей**. Бізнес хотів би використовувати точно налаштовані менш здатні моделі (як моделі вкладень) замість високопродуктивних моделей, що призводить до більш економічно ефективного та швидкого рішення.

- **Розгляд затримки**. Затримка важлива для конкретного випадку використання, тому неможливо використовувати дуже довгі промпти або кількість прикладів, які повинна вивчити модель, не вписується в обмеження довжини промпту.

- **Підтримка актуальності**. Бізнес має багато якісних даних та еталонних міток, а також ресурси, необхідні для підтримки цих даних в актуальному стані з часом.

### Навчена модель

Навчання LLM з нуля, без сумніву, є найскладнішим і найкомплекснішим підходом для впровадження, що вимагає величезної кількості даних, кваліфікованих ресурсів та відповідної обчислювальної потужності. Цей варіант слід розглядати лише в сценарії, коли бізнес має специфічний для домену випадок використання та велику кількість даних, орієнтованих на домен.

## Перевірка знань

Який міг би бути хорошим підходом для покращення результатів завершення LLM?

1. Інженерія промптів з контекстом
1. RAG
1. Точно налаштована модель

В:3, якщо у вас є час, ресурси та якісні дані, точне налаштування є кращим варіантом для підтримки актуальності. Однак, якщо ви шукаєте способи покращення і у вас не вистачає часу, варто спочатку розглянути RAG.

## 🚀 Завдання

Прочитайте більше про те, як ви можете [використовувати RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) для вашого бізнесу.

## Чудова робота, продовжуйте навчання

Після завершення цього уроку, перегляньте нашу [колекцію навчальних матеріалів з Генеративного ШІ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб продовжити підвищувати свої знання про Генеративний ШІ!

Переходьте до Уроку 3, де ми розглянемо, як [будувати з Генеративним ШІ відповідально](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!