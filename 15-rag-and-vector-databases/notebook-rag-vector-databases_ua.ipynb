{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Розширення генерації на основі пошуку (RAG) та векторні бази даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mistralai getenv openai faiss-cpu pandas numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Створення нашої бази знань\n",
    "\n",
    "Налаштування FAISS для векторного пошуку\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шляхи до файлів (даних для RAG)\n",
    "data_paths = [\n",
    "    \"data/frameworks.md\", \n",
    "    \"data/own_framework.md\", \n",
    "    \"data/perceptron.md\"\n",
    "] \n",
    "\n",
    "# Ініціалізація порожнього DataFrame\n",
    "df = pd.DataFrame(columns=['path', 'text'])\n",
    "\n",
    "# Сучасний спосіб додавання рядків до DataFrame\n",
    "for path in data_paths:\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "        \n",
    "        # Використовуємо concat замість застарілого append\n",
    "        new_row = pd.DataFrame({'path': [path], 'text': [file_content]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл не знайдено: {path}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length, min_length):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    # Якщо останній фрагмент не досягнув мінімальної довжини, все одно додати його\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Припускаючи, що analyzed_df - це pandas DataFrame, а 'output_content' - це стовпець у цьому DataFrame\n",
    "splitted_df = df.copy()\n",
    "splitted_df['chunks'] = splitted_df['text'].apply(lambda x: split_text(x, 400, 300))\n",
    "\n",
    "splitted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Припускаючи, що 'chunks' - це стовпець списків у DataFrame splitted_df, ми розділимо фрагменти на різні рядки\n",
    "flattened_df = splitted_df.explode('chunks')\n",
    "\n",
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перетворення нашого тексту на ембедінги\n",
    "\n",
    "Перетворення нашого тексту на ембедінги із використанням сервісу [MistralAI](https:/docs.mistral.ai/capabilities/embeddings/) та зберігання їх для використання з FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Ініціалізація клієнта Mistral\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "assert api_key, \"ERROR: MISTRAL_API_KEY is missing\"\n",
    "\n",
    "client_embedding = Mistral(api_key=api_key)\n",
    "\n",
    "def create_embeddings(text, model=\"mistral-embed\"):\n",
    "    \"\"\"\n",
    "    Створює ембедінги для тексту, використовуючи Mistral AI.\n",
    "    \n",
    "    Args:\n",
    "        text: Текст або список текстів для ембедінгу\n",
    "        model: Модель для ембедінгу (за замовчуванням mistral-embed)\n",
    "        \n",
    "    Returns:\n",
    "        Вектор ембедінгу\n",
    "    \"\"\"\n",
    "    # Обробка pandas Series\n",
    "    if isinstance(text, pd.Series):\n",
    "        # Беремо перший елемент з Series\n",
    "        text = text.iloc[0]\n",
    "    \n",
    "    # Перетворюємо в список рядків для API\n",
    "    if not isinstance(text, list):\n",
    "        text = [str(text)]\n",
    "    else:\n",
    "        text = [str(item) for item in text]\n",
    "    \n",
    "    # Виклик API Mistral для отримання ембедінгів\n",
    "    embeddings_response = client_embedding.embeddings.create(\n",
    "        model=model,\n",
    "        inputs=text\n",
    "    )\n",
    "    \n",
    "    # Повернення ембедінгу для першого елемента\n",
    "    return embeddings_response.data[0].embedding\n",
    "\n",
    "# Приклад використання:\n",
    "embeddings = create_embeddings(flattened_df['chunks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = create_embeddings(\"cat\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_embeddings(df, folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    Зберігає DataFrame з ембедінгами в указану папку.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame з ембедінгами\n",
    "        folder: Назва папки для збереження\n",
    "        filename: Ім'я файлу для збереження\n",
    "    \"\"\"\n",
    "    # Створення директорії, якщо вона не існує\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Шлях до файлу\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    # Збереження DataFrame\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    \n",
    "    print(f\"DataFrame успішно збережено в {file_path}\")\n",
    "\n",
    "def load_embeddings(folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    Завантажує DataFrame з ембедінгами з указаної папки.\n",
    "    \n",
    "    Args:\n",
    "        folder: Назва папки для завантаження\n",
    "        filename: Ім'я файлу для завантаження\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame з ембедінгами або None, якщо файл не існує\n",
    "    \"\"\"\n",
    "    # Шлях до файлу\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    # Перевірка існування файлу\n",
    "    if os.path.exists(file_path):\n",
    "        # Завантаження DataFrame\n",
    "        with open(file_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        \n",
    "        print(f\"DataFrame успішно завантажено з {file_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Файл {file_path} не знайдено\")\n",
    "        return None\n",
    "\n",
    "def get_or_create_embeddings(df, chunk_column, embedding_function, folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    Завантажує DataFrame з ембедінгами або створює новий.\n",
    "    \n",
    "    Args:\n",
    "        df: Вихідний DataFrame з текстами\n",
    "        chunk_column: Назва стовпця з текстовими фрагментами\n",
    "        embedding_function: Функція для створення ембедінгів\n",
    "        folder: Назва папки для збереження/завантаження\n",
    "        filename: Ім'я файлу для збереження/завантаження\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame з ембедінгами\n",
    "    \"\"\"\n",
    "    # Спроба завантажити DataFrame\n",
    "    loaded_df = load_embeddings(folder, filename)\n",
    "    \n",
    "    if loaded_df is not None:\n",
    "        return loaded_df\n",
    "    \n",
    "    # Якщо завантаження не вдалося, створюємо ембедінги\n",
    "    print(\"Створення нових ембедінгів...\")\n",
    "    \n",
    "    # Створення ембедінгів\n",
    "    embeddings = []\n",
    "    for chunk in df[chunk_column]:\n",
    "        embeddings.append(embedding_function(chunk))\n",
    "    \n",
    "    # Збереження ембедінгів в DataFrame\n",
    "    df['embeddings'] = embeddings\n",
    "    \n",
    "    # Збереження DataFrame\n",
    "    save_embeddings(df, folder, filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Використовуємо функцію, яка розраховує ембедінги, \n",
    "# якщо вони не були раніше створені і збережені в папці в \"15-rag-and-vector-databases/embeddings\"\n",
    "flattened_df = get_or_create_embeddings(\n",
    "    splitted_df.explode('chunks'), \n",
    "    'chunks', \n",
    "    create_embeddings\n",
    ")\n",
    "\n",
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пошук з використанням FAISS\n",
    "\n",
    "Векторний пошук та схожість між нашим запитом і базою даних з використанням FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Створення індексу FAISS та підготовка до пошуку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отримуємо ембедінги як масив numpy\n",
    "embeddings_list = flattened_df['embeddings'].to_list()\n",
    "embeddings_array = np.array(embeddings_list).astype('float32')\n",
    "\n",
    "# Визначаємо розмірність векторів\n",
    "vector_dimension = len(embeddings_list[0])\n",
    "\n",
    "# Створюємо індекс FAISS\n",
    "index = faiss.IndexFlatL2(vector_dimension)  # L2 - це евклідова відстань\n",
    "\n",
    "# Додаємо наші вектори до індексу\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Перевіряємо кількість векторів в індексі\n",
    "print(f\"Загальна кількість векторів в індексі: {index.ntotal}, розмірність векторів: {vector_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше текстове запитання\n",
    "question = \"Що таке персептрон?\"\n",
    "\n",
    "# Перетворіть запитання у вектор запиту\n",
    "query_vector = create_embeddings(question)  \n",
    "query_vector_array = np.array([query_vector]).astype('float32')\n",
    "\n",
    "# Знайдіть найбільш схожі документи (k=5 - скільки найближчих сусідів шукаємо)\n",
    "k = 5\n",
    "distances, indices = index.search(query_vector_array, k)\n",
    "\n",
    "# Виведіть найбільш схожі документи\n",
    "for i in range(min(3, len(indices[0]))):\n",
    "    idx = indices[0][i]\n",
    "    print(f\"Фрагмент {i+1}:\")\n",
    "    print(flattened_df['chunks'].iloc[idx])\n",
    "    print(f\"Шлях: {flattened_df['path'].iloc[idx]}\")\n",
    "    print(f\"Відстань: {distances[0][i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поєднання всього для відповіді на запитання"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import ChatCompletionsToolDefinition\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "# Виберіть модель загального призначення для тексту\n",
    "deployment = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реалізація чатботів (при наявності і відсутності RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_with_rag(user_input):\n",
    "    # Перетворіть запитання у вектор запиту\n",
    "    query_vector = create_embeddings(user_input)\n",
    "    query_vector_array = np.array([query_vector]).astype('float32')\n",
    "    \n",
    "    # Знайдіть найбільш схожі документи з FAISS\n",
    "    k = 5  # кількість найближчих сусідів для пошуку\n",
    "    distances, indices = index.search(query_vector_array, k)\n",
    "\n",
    "    # додайте документи до запиту, щоб забезпечити контекст\n",
    "    history = []\n",
    "    for idx in indices[0]:\n",
    "        history.append(flattened_df['chunks'].iloc[idx])\n",
    "\n",
    "    # створюємо об'єкт повідомлення з контекстом\n",
    "    context = \"\\n\\n\".join(history)  # всі знайдені фрагменти\n",
    "\n",
    "    # Формуємо запит, що просить коротку, але завершену відповідь\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with AI questions. \"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\\n\\n Provide a brief but complete answer based on the context. Answer in Ukrainian.\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = client.complete(\n",
    "        temperature=0,\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def chatbot_without_rag(user_input):\n",
    "    \"\"\"\n",
    "    Чат-бот без використання RAG (прямий запит до моделі).\n",
    "    \"\"\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with AI questions. Provide brief but complete answers. Answer in Ukrainian.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {user_input}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.complete(\n",
    "        temperature=0,\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порівняння відповідей RAG-системи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def compare_responses(user_input, save_to_file=False, filename=\"rag_comparison.md\"):\n",
    "    \"\"\"\n",
    "    Порівнює відповіді чат-боту з RAG та без RAG.\n",
    "    \n",
    "    Args:\n",
    "        user_input: Запитання користувача\n",
    "        save_to_file: Зберегти результат у файл Markdown\n",
    "        filename: Назва файлу для збереження\n",
    "    \"\"\"\n",
    "    # Отримання знайдених чанків\n",
    "    query_vector = create_embeddings(user_input)\n",
    "    query_vector_array = np.array([query_vector]).astype('float32')\n",
    "    k = 5  # кількість найближчих сусідів для пошуку\n",
    "    distances, indices = index.search(query_vector_array, k)\n",
    "    \n",
    "    # Отримання відповідей\n",
    "    rag_response = chatbot_with_rag(user_input)\n",
    "    no_rag_response = chatbot_without_rag(user_input)\n",
    "    \n",
    "    # Формування markdown-тексту\n",
    "    markdown_text = f\"\"\"\n",
    "# Порівняння відповідей\n",
    "\n",
    "## 📝 Запит: {user_input}\n",
    "\n",
    "## 🔍 Відповіді моделей\n",
    "\n",
    "### Без використання RAG\n",
    "\n",
    "{no_rag_response}\n",
    "\n",
    "### З використанням RAG\n",
    "\n",
    "{rag_response}\n",
    "\n",
    "## 📚 Знайдені фрагменти тексту\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Додавання чанків\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        chunk_content = flattened_df['chunks'].iloc[idx]\n",
    "        path = flattened_df['path'].iloc[idx]\n",
    "        dist = float(distances[0][i])\n",
    "        \n",
    "        markdown_text += f\"\"\"\n",
    "### Фрагмент {i+1} (відстань: {dist:.4f})\n",
    "\n",
    "**Шлях**: {path}\n",
    "\n",
    "{chunk_content}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Виведення Markdown\n",
    "    display(Markdown(markdown_text))\n",
    "    \n",
    "    # Для коректного відображення формул\n",
    "    mathjax_script = \"\"\"\n",
    "    <script type=\"text/javascript\">\n",
    "        MathJax = {\n",
    "            tex: {\n",
    "                inlineMath: [['$', '$']]\n",
    "            }\n",
    "        };\n",
    "    </script>\n",
    "    <script type=\"text/javascript\" id=\"MathJax-script\" async\n",
    "        src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js\">\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    display(HTML(mathjax_script))\n",
    "    \n",
    "    # Збереження в файл, якщо потрібно\n",
    "    if save_to_file:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        print(f\"Результати збережено у файл: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приклад використання:\n",
    "compare_responses(\"Що таке перцептрон?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
