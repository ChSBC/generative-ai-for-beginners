# Генерація з Використанням Пошуку (RAG) та Векторні Бази Даних

[![Генерація з Використанням Пошуку (RAG) та Векторні Бази Даних](./images/15-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

У уроці про пошукові застосунки, ми коротко ознайомилися з тим, як інтегрувати власні дані в Великі Мовні Моделі (LLM). У цьому уроці ми глибше розглянемо концепції обґрунтування ваших даних у вашому LLM-застосунку, механіку процесу та методи зберігання даних, включаючи як вбудовування, так і текст.

> **Відео незабаром**

## Вступ

У цьому уроці ми охопимо наступне:

- Вступ до RAG, що це таке і для чого використовується в ШІ (штучному інтелекті).

- Розуміння того, що таке векторні бази даних та створення однієї для нашого застосунку.

- Практичний приклад того, як інтегрувати RAG в застосунок.

## Цілі навчання

Після завершення цього уроку ви зможете:

- Пояснити значення RAG у пошуку та обробці даних.

- Налаштувати RAG-застосунок та обґрунтувати ваші дані для LLM

- Ефективно інтегрувати RAG та Векторні Бази Даних в LLM-застосунках.

## Наш сценарій: вдосконалення наших LLM власними даними

Для цього уроку ми хочемо додати наші власні нотатки до освітнього стартапу, що дозволить чатботу отримувати більше інформації про різні предмети. Використовуючи наші нотатки, учні зможуть краще навчатися та розуміти різні теми, що полегшить підготовку до іспитів. Для створення нашого сценарію ми використаємо:

- `Azure OpenAI:` LLM, яку ми використовуватимемо для створення нашого чатботу

- `Урок про Нейронні Мережі з курсу "AI for beginners"`: це будуть дані, на які ми обґрунтуємо нашу LLM

- `Azure AI Search` та `Azure Cosmos DB:` векторна база даних для зберігання наших даних та створення пошукового індексу

Користувачі зможуть створювати практичні тести зі своїх нотаток, картки для повторення та узагальнювати їх до стислих оглядів. Щоб почати, давайте розглянемо, що таке RAG і як він працює:

## Генерація з Використанням Пошуку (RAG)

Чатбот, що працює на LLM, обробляє запити користувачів для генерації відповідей. Він розроблений для інтерактивної взаємодії та спілкування з користувачами на широкий спектр тем. Однак його відповіді обмежені наданим контекстом та його базовими навчальними даними. Наприклад, дата обмеження знань GPT-4 - вересень 2021 року, що означає, що йому бракує знань про події, які відбулися після цього періоду. Крім того, дані, що використовуються для навчання LLM, виключають конфіденційну інформацію, таку як особисті нотатки чи посібник з використання продуктів компанії.

### Як працюють RAG (Генерація з Використанням Пошуку)

![малюнок, що показує як працюють RAG](images/how-rag-works.png?WT.mc_id=academic-105485-koreyst)

Припустімо, ви хочете розгорнути чатбот, який створює тести з ваших нотаток, вам знадобиться з'єднання з базою знань. Ось тут на допомогу приходить RAG. RAG працює наступним чином:

- **База знань:** Перед пошуком ці документи потрібно завантажити та попередньо обробити, зазвичай розбиваючи великі документи на менші фрагменти, перетворюючи їх на текстові вбудовування та зберігаючи їх у базі даних.

- **Запит користувача:** користувач задає питання

- **Пошук:** Коли користувач задає питання, модель вбудовування отримує релевантну інформацію з нашої бази знань, щоб надати більше контексту, який буде включено до запиту.

- **Генерація з Використанням Пошуку:** LLM покращує свою відповідь на основі отриманих даних. Це дозволяє генерувати відповідь не лише на основі попередньо навчених даних, але й на основі релевантної інформації з доданого контексту. Отримані дані використовуються для збагачення відповідей LLM. Потім LLM повертає відповідь на запитання користувача.

![малюнок, що показує архітектуру RAG](images/encoder-decode.png?WT.mc_id=academic-105485-koreyst)

Архітектура для RAG реалізована з використанням трансформерів, що складається з двох частин: кодувальника та декодувальника. Наприклад, коли користувач задає питання, вхідний текст 'кодується' у вектори, що захоплюють значення слів, а вектори 'декодуються' в наш індекс документів і генерують новий текст на основі запиту користувача. LLM використовує як модель кодувальник-декодувальник для генерації вихідних даних.

Два підходи при впровадженні RAG, згідно із запропонованою роботою: [Генерація з Використанням Пошуку для Завдань з Інтенсивним Використанням Знань у NLP (програмне забезпечення для обробки природної мови)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) є:

- **_RAG-Sequence_** (послідовність) використовує отримані документи для прогнозування найкращої можливої відповіді на запит користувача

- **RAG-Token** (токен) використовує документи для генерації наступного токену, а потім отримує їх для відповіді на запит користувача

### Навіщо використовувати RAG? 

- **Інформаційне багатство:** забезпечує актуальність та відповідність текстових відповідей. Таким чином, він покращує продуктивність у специфічних завданнях домену, отримуючи доступ до внутрішньої бази знань.

- Зменшує фабрикацію, використовуючи **перевірені дані** в базі знань для забезпечення контексту для запитів користувачів.

- Це **економічно ефективно**, оскільки вони більш економічні порівняно з тонким налаштуванням LLM

## Створення бази знань

Наш застосунок базується на наших особистих даних, тобто уроці про Нейронні Мережі з курсу AI For Beginners.

### Векторні Бази Даних

Векторна база даних, на відміну від традиційних баз даних, це спеціалізована база даних, розроблена для зберігання, управління та пошуку вбудованих векторів. Вона зберігає числові представлення документів. Розбиття даних на числові вбудовування полегшує нашій системі ШІ розуміння та обробку даних.

Ми зберігаємо наші вбудовування у векторних базах даних, оскільки LLM мають обмеження на кількість токенів, які вони приймають як вхідні дані. Оскільки ви не можете передати всі вбудовування в LLM, нам потрібно буде розбити їх на фрагменти, і коли користувач задає питання, вбудовування, найбільш схожі на питання, будуть повернуті разом із запитом. Розбиття на фрагменти також зменшує витрати на кількість токенів, переданих через LLM.

Деякі популярні векторні бази даних включають Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant та DeepLake. Ви можете створити модель Azure Cosmos DB за допомогою Azure CLI з наступною командою:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Від тексту до вбудовувань

Перш ніж ми зберігатимемо наші дані, нам потрібно буде перетворити їх на векторні вбудовування перед збереженням у базі даних. Якщо ви працюєте з великими документами або довгими текстами, ви можете розбити їх на фрагменти на основі очікуваних запитів. Розбиття на фрагменти може бути здійснене на рівні речень або на рівні абзаців. Оскільки розбиття на фрагменти виводить значення зі слів навколо них, ви можете додати деякий інший контекст до фрагменту, наприклад, додаючи заголовок документа або включаючи деякий текст перед або після фрагменту. Ви можете розбити дані на фрагменти наступним чином:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Після розбиття на фрагменти, ми можемо вбудувати наш текст, використовуючи різні моделі вбудовування. Деякі моделі, які ви можете використовувати, включають: word2vec, ada-002 від OpenAI, Azure Computer Vision та багато інших. Вибір моделі залежатиме від мов, які ви використовуєте, типу кодованого вмісту (текст/зображення/аудіо), розміру введення, який вона може кодувати, та довжини вихідного вбудовування.

Приклад вбудованого тексту з використанням моделі `text-embedding-ada-002` від OpenAI:
![вбудовування слова cat](images/cat.png?WT.mc_id=academic-105485-koreyst)

## Пошук та Векторний Пошук

Коли користувач задає питання, пошуковик перетворює його на вектор за допомогою кодувальника запитів, потім шукає в нашому індексі пошуку документів релевантні вектори в документі, які пов'язані з вхідними даними. Після цього він перетворює як вхідний вектор, так і вектори документів на текст і передає його через LLM.

### Пошук

Пошук відбувається, коли система намагається швидко знайти документи з індексу, які відповідають критеріям пошуку. Метою пошуковика є отримання документів, які будуть використовуватися для надання контексту та обґрунтування LLM на ваших даних.

Існує кілька способів виконання пошуку в нашій базі даних, таких як:

- **Пошук за ключовими словами** - використовується для текстових пошуків

- **Семантичний пошук** - використовує семантичне значення слів

- **Векторний пошук** - перетворює документи з тексту на векторні представлення, використовуючи моделі вбудовування. Пошук буде здійснюватися шляхом запиту документів, векторні представлення яких найближчі до питання користувача.

- **Гібридний** - комбінація пошуку за ключовими словами та векторного пошуку.

Виклик з пошуком виникає, коли в базі даних немає подібної відповіді на запит, система тоді поверне найкращу інформацію, яку вони можуть отримати, однак, ви можете використовувати тактики, такі як встановлення максимальної відстані для релевантності або використання гібридного пошуку, що поєднує як пошук за ключовими словами, так і векторний пошук. У цьому уроці ми будемо використовувати гібридний пошук, комбінацію векторного пошуку та пошуку за ключовими словами. Ми зберігатимемо наші дані у DataFrame з колонками, що містять фрагменти, а також вбудовування.

### Векторна Схожість

Пошуковик шукатиме в базі знань вбудовування, які знаходяться близько одне до одного, найближчого сусіда, оскільки це тексти, які схожі. У сценарії, коли користувач задає запит, він спершу вбудовується, а потім зіставляється з подібними вбудовуваннями. Поширеним методом, який використовується для визначення схожості різних векторів, є косинусна схожість, що базується на куті між двома векторами.

Ми можемо вимірювати схожість, використовуючи інші альтернативи, такі як Евклідова відстань, яка є прямою лінією між кінцевими точками векторів, та скалярний добуток, який вимірює суму добутків відповідних елементів двох векторів.

### Пошуковий індекс

При виконанні пошуку нам потрібно буде створити пошуковий індекс для нашої бази знань перед тим, як виконувати пошук. Індекс зберігатиме наші вбудовування і зможе швидко отримати найбільш схожі фрагменти навіть у великій базі даних. Ми можемо створити наш індекс локально за допомогою:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Перерозподіл рангу

Після запиту до бази даних, можливо, вам знадобиться відсортувати результати від найбільш релевантних. Перерозподільник рангу LLM використовує машинне навчання для покращення релевантності результатів пошуку, впорядковуючи їх від найбільш релевантних. Використовуючи Azure AI Search, перерозподіл рангу виконується автоматично за допомогою семантичного перерозподільника рангу. Приклад того, як працює перерозподіл рангу з використанням найближчих сусідів:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Об'єднуємо все разом

Останнім кроком є додавання нашої LLM до суміші, щоб отримати відповіді, які обґрунтовані на наших даних. Ми можемо реалізувати це наступним чином:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Оцінювання нашого застосунку

### Метрики Оцінювання

- Якість наданих відповідей, забезпечуючи їх природність, плавність та людиноподібність

- Обґрунтованість даних: оцінка, чи відповідь походить з наданих документів

- Релевантність: оцінка, чи відповідь відповідає та пов'язана з поставленим питанням

- Плавність - чи має відповідь граматичний сенс

## Випадки Використання RAG (Генерації з Використанням Пошуку) та Векторних Баз Даних

Існує багато різних випадків використання, де функціональні виклики можуть покращити ваш застосунок, такі як:

- Запитання та Відповіді: обґрунтування даних вашої компанії для чату, який можуть використовувати співробітники для задавання питань.

- Рекомендаційні Системи: де ви можете створити систему, яка зіставляє найбільш схожі значення, наприклад, фільми, ресторани та багато іншого.

- Чатбот сервіси: ви можете зберігати історію чатів та персоналізувати розмову на основі даних користувача.

- Пошук зображень на основі векторних вбудовувань, корисний при розпізнаванні зображень та виявленні аномалій.

## Підсумок

Ми охопили фундаментальні області RAG, від додавання наших даних до застосунку, запиту користувача та виходу. Для спрощення створення RAG, ви можете використовувати фреймворки, такі як Semanti Kernel, Langchain або Autogen.

## Завдання

Щоб продовжити ваше вивчення Генерації з Використанням Пошуку (RAG), ви можете побудувати:

- Побудуйте фронтенд для застосунку, використовуючи фреймворк на ваш вибір

- Використайте фреймворк, або LangChain, або Semantic Kernel, і перестворіть ваш застосунок.

Вітаємо з завершенням уроку 👏.

## Навчання не закінчується тут, продовжуйте Подорож

Після завершення цього уроку, перегляньте нашу [колекцію навчання з Генеративного ШІ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб продовжувати підвищувати ваші знання з Генеративного ШІ!